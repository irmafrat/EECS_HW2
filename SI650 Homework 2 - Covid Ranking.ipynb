{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "Please see the write-up on Canvas for full details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install metapy, it may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "XjUUOguhUCgE",
    "outputId": "b3ef1398-5b5c-488f-e06e-267a465d36ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting metapy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/a4/92dae084446597d6bbf355e7eaff3e83dcb51e33d434f43ecdea4c0c4b0a/metapy-0.2.13-cp36-cp36m-manylinux1_x86_64.whl (14.3MB)\n",
      "\u001b[K     |████████████████████████████████| 14.3MB 306kB/s \n",
      "\u001b[?25hInstalling collected packages: metapy\n",
      "Successfully installed metapy-0.2.13\n"
     ]
    }
   ],
   "source": [
    "!pip install metapy\n",
    "import metapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the dataset files for this assignment. \n",
    "\n",
    "These files are also on canvas. The `wget` and `tar` commands may not work on Windows, so if this command doesn't work, just download them on canvas. These commands should work if you're doing the assignment on Google Collab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "pWwB7o8zUHAV",
    "outputId": "ffd64367-a734-4ca2-f00f-ef9796b9185a"
   },
   "outputs": [],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/meta-toolkit/meta/master/data/lemur-stopwords.txt\n",
    "!wget -N http://www-personal.umich.edu/~shiyansi/covid_ir.tar.gz\n",
    "!tar xf covid_ir.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the metapy header configuration for you\n",
    "Metapy is a powerful IR library. To lower the barrier for entry, we're generating the configuration that tells metapy how the task is setup and what is needed. You should keep this file the same when verifying your BM25 implementation in Part 1 of the assignment.  However, you can generate a different version of it if you want when trying to outperform BM25 in Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R1mkpb7oUJ3S"
   },
   "outputs": [],
   "source": [
    "with open('covid_ir/tutorial.toml', 'w') as f:\n",
    "    f.write('type = \"line-corpus\"\\n')\n",
    "    f.write('store-full-text = true\\n')\n",
    "\n",
    "config = \"\"\"prefix = \".\" # tells MeTA where to search for datasets\n",
    "\n",
    "dataset = \"covid_ir\" # a subfolder under the prefix directory\n",
    "corpus = \"tutorial.toml\" # a configuration file for the corpus specifying its format & additional args\n",
    "\n",
    "index = \"covid_ir-idx\" # subfolder of the current working directory to place index files\n",
    "\n",
    "query-judgements = \"covid_ir/covid_ir-qrels.txt\" # file containing the relevance judgments for this dataset\n",
    "\n",
    "stop-words = \"lemur-stopwords.txt\"\n",
    "\n",
    "[[analyzers]]\n",
    "method = \"ngram-word\"\n",
    "ngram = 1\n",
    "filter = \"default-unigram-chain\"\n",
    "\"\"\"\n",
    "with open('covid_ir-config.toml', 'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the inverted index with metapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztJSZMExVkMW"
   },
   "outputs": [],
   "source": [
    "inv_idx = metapy.index.make_inverted_index('covid_ir-config.toml') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Re-implemented BM25 (25 points) and Pivoted Indexing (25 points)\n",
    "We've provided a skeleton of a ranking function below with examples of commonly-used parameters. For each, you should re-implement it using the formulas as defined in the lecture slides. You are welcome to use any hyperparameter choices you want (NOTE: as mentioned in the homework, changing these does not count as a new method for Problem 2). \n",
    "\n",
    "To test for correctness, you can compare your method against metapy's implementations. We've included one below to get you started. Your solution should return the exact same ranking as their implementation when using identical hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r2-p9AGfV1Ia"
   },
   "outputs": [],
   "source": [
    "# You can define your own retrieval function \n",
    "import math \n",
    "class MyBM25Reimplementation(metapy.index.RankingFunction):                                                                                                                    \n",
    "    def __init__(self, s = 0.1):                                             \n",
    "        self.s = s\n",
    "        # You *must* invoke the base class __init__() here!\n",
    "        super(Pivoted, self).__init__()                                        \n",
    "                                                                                 \n",
    "    def score_one(self, sd):\n",
    "        \"\"\"\n",
    "        You need to override this function to return a score for a single term.\n",
    "        \n",
    "        You may want to call some of the following variables when implementing your retrieval function:\n",
    "        \n",
    "        sd.avg_dl: average document length of the collection\n",
    "        sd.num_docs: total number of documents in the index\n",
    "        sd.total_terms: total number of terms in the index\n",
    "        sd.query_length: the total length of the current query (sum of all term weights)\n",
    "        sd.query_term_weight: query term count (or weight in case of feedback)\n",
    "        sd.doc_count: number of documents that a term t_id appears in\n",
    "        sd.corpus_term_count: number of times a term t_id appears in the collection\n",
    "        sd.doc_term_count: number of times the term appears in the current document\n",
    "        sd.doc_size: total number of terms in the current document\n",
    "        sd.doc_unique_terms: number of unique terms in the current document\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        s = self.s      \n",
    "        #Fill your answers here\n",
    "\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTxPdqbyVoWj"
   },
   "outputs": [],
   "source": [
    "ranker = metapy.index.OkapiBM25(k1 = 1.2, b = 0.5, k3 = 500)\n",
    "# ranker = MyBM25Reimplementation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4L9nOa6BVrPF"
   },
   "outputs": [],
   "source": [
    "num_results = 10\n",
    "retrieval_results = []\n",
    "with open('covid_ir/covid_ir-queries.txt') as query_file:\n",
    "    for query_num, line in enumerate(query_file):\n",
    "        query = metapy.index.Document()\n",
    "        query.content(line.strip())\n",
    "        results = ranker.score(inv_idx, query, num_results)  \n",
    "        res_list = [(query_num + 1, x[0]) for x in results]\n",
    "        retrieval_results += res_list\n",
    "\n",
    "        \n",
    "        # print(\"Query: \", query.content())\n",
    "        # print(\"Retrieved Results\")\n",
    "        # for num, (d_id, _) in enumerate(results):\n",
    "        #    content = inv_idx.metadata(d_id).get('content')\n",
    "        #    print(str(num + 1), content)\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9mMaxMifWkWT"
   },
   "outputs": [],
   "source": [
    "# You can check your results by comparing the two rankers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k6auOVd-YEuG"
   },
   "source": [
    "## Part 2: Define your own ranking function! (50 points)\n",
    "Implement at least one retrieval function *different* from BM25, Dirichlet Prior, and Pivoted Normalization. You will be graded based on your best performing function. You’ll get full credit if your retrieval function can beat the provided baseline in the dataset. By \"beat,\" we mean that your implemented function and your choice of parameters should reach higher NDCG@10 than the baseline on Kaggle for our dataset, which you can check at any time. Report this information in your submission: the code to implement the retrieval function, the parameter you used that achieved the best performance, and the best performance. In addition,_explain_ what you have explored and why you decide to try those. You will lose points if you cannot explain why your function can reach a higher performance. You can include your explanations in the end of the submitted notebook.\n",
    "\n",
    "*Note:* Simply varying the value of parameters in Okapi/BM25, Dirichlet Prior or Pivoted Normalization does not count as a new retrieval function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can define your own retrieval function \n",
    "import math \n",
    "class MyCustomRanker(metapy.index.RankingFunction):                                                                                                                    \n",
    "    def __init__(self, s = 0.1):                                             \n",
    "        self.s = s\n",
    "        # You *must* invoke the base class __init__() here!\n",
    "        super(Pivoted, self).__init__()                                        \n",
    "                                                                                 \n",
    "    def score_one(self, sd):\n",
    "        \"\"\"\n",
    "        You need to override this function to return a score for a single term.\n",
    "        \n",
    "        You may want to call some of the following variables when implementing your retrieval function:\n",
    "        \n",
    "        sd.avg_dl: average document length of the collection\n",
    "        sd.num_docs: total number of documents in the index\n",
    "        sd.total_terms: total number of terms in the index\n",
    "        sd.query_length: the total length of the current query (sum of all term weights)\n",
    "        sd.query_term_weight: query term count (or weight in case of feedback)\n",
    "        sd.doc_count: number of documents that a term t_id appears in\n",
    "        sd.corpus_term_count: number of times a term t_id appears in the collection\n",
    "        sd.doc_term_count: number of times the term appears in the current document\n",
    "        sd.doc_size: total number of terms in the current document\n",
    "        sd.doc_unique_terms: number of unique terms in the current document\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        s = self.s      \n",
    "        #Fill your answers here\n",
    "\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = MyCustomRanker()\n",
    "\n",
    "num_results = 10\n",
    "custom_ranking_retrieval_results = []\n",
    "with open('covid_ir/covid_ir-queries.txt') as query_file:\n",
    "    for query_num, line in enumerate(query_file):\n",
    "        query = metapy.index.Document()\n",
    "        query.content(line.strip())\n",
    "        results = ranker.score(inv_idx, query, num_results)  \n",
    "        res_list = [(query_num + 1, x[0]) for x in results]\n",
    "        custom_ranking_retrieval_results += res_list\n",
    "\n",
    "        \n",
    "        # print(\"Query: \", query.content())\n",
    "        # print(\"Retrieved Results\")\n",
    "        # for num, (d_id, _) in enumerate(results):\n",
    "        #    content = inv_idx.metadata(d_id).get('content')\n",
    "        #    print(str(num + 1), content)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your ranking to a file and upload it to the [Kaggle competition](https://www.kaggle.com/t/a8345852fdab42da9e210f833b9f50b1) for the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"my_kaggle_submission.csv\",\"w\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow([\"queryid\", \"docid\"])\n",
    "    for x in retrieval_results:\n",
    "        custom_ranking_retrieval_results.writerow(list(x))\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Covid19_IR_Trial",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
